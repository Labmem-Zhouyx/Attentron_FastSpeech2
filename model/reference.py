import math
import torch
import torch.nn as nn
import torch.nn.functional as F
from utils.tools import get_mask_from_lengths


class ReferenceEncoder(nn.Module):
    def __init__(self, in_dim=80, conv_channels=[512, 512], conv_kernel_size=3, conv_stride=1, conv_padding=1, lstm_layer_num=2, lstm_hidden_size=512):
        super(ReferenceEncoder, self).__init__()

        # convolution layers followed by batch normalization and ReLU activation
        K = len(conv_channels)
        
        # 1-D convolution layers
        filters = [in_dim] + conv_channels

        self.conv1ds = nn.ModuleList(
            [nn.Conv1d(in_channels=filters[i],
                       out_channels=filters[i+1],
                       kernel_size=conv_kernel_size,
                       stride=conv_stride,
                       padding=conv_padding)
             for i in range(K)])

        # 1-D batch normalization (BN) layers
        self.bns = nn.ModuleList(
            [nn.BatchNorm1d(num_features=conv_channels[i])
             for i in range(K)])

        # ReLU
        self.relu = nn.ReLU()

        self.lstm = nn.LSTM(input_size=conv_channels[-1],
                            hidden_size=lstm_hidden_size // 2,
                            num_layers=lstm_layer_num,
                            batch_first=True,
                            bidirectional=True,
                            )


    def forward(self, x):
        # transpose to (B, embed_dim, T) for convolution, and then back
        out = x.transpose(1, 2)
        for conv, bn in zip(self.conv1ds, self.bns):
            out = conv(out)
            out = self.relu(out)
            out = bn(out)  # [B, 128, T//2^K, mel_dim//2^K], where 128 = conv_channels[-1]

        out = out.transpose(1, 2)  # [B, T//2^K, 128, mel_dim//2^K]
        B, T = out.size(0), out.size(1)
        out = out.contiguous().view(B, T, -1)  # [B, T//2^K, 128*mel_dim//2^K]

        self.lstm.flatten_parameters()
        out, _ = self.lstm(out)

        return out


class ReferenceAttention(nn.Module):
    '''
    embedded_text --- [N, seq_len, text_embedding_dim]
    mels --- [N, n_mels*r, Ty/r], r=1
    style_embed --- [N, seq_len, style_embedding_dim]
    alignments --- [N, seq_len, ref_len], Ty/r = ref_len
    '''
    def __init__(self, query_dim=256, key_dim=256, ref_attention_dim=128, ref_attention_dropout=0):
        super(ReferenceAttention, self).__init__()
        self.attn = ScaledDotProductAttention(query_dim, key_dim, ref_attention_dim, ref_attention_dropout)

    def forward(self, text_embeddings, text_lengths, key, value, ref_mels, ref_mel_lengths):

        if text_lengths == None or ref_mel_lengths == None:
            attn_mask = None
        else:
            # Get attention mask
            # 1. text mask
            text_total_length = text_embeddings.size(1)  # [N, T_x, dim]
            text_mask = get_mask_from_lengths(text_lengths, text_total_length).float().unsqueeze(-1)  # [B, seq_len, 1]
            # 2. mel mask (regularized to phoneme_scale)
            ref_mel_total_length = ref_mels.size(1)  # [N, T_y, n_mels]
            ref_mel_mask = get_mask_from_lengths(ref_mel_lengths, ref_mel_total_length).float().unsqueeze(-1)  # [B, rseq_len, 1]
            ref_mel_mask = F.interpolate(ref_mel_mask.transpose(1, 2), size=key.size(1))  # [B, 1, Ty]
            # 3. The attention mask
           
            attn_mask = 1 - torch.bmm(text_mask, ref_mel_mask)  # [N, seq_len, ref_len]
            

        # Attention
        style_embed, alignments = self.attn(text_embeddings, key, value, attn_mask)

        # Apply ReLU as the activation function to force the values of the prosody embedding to lie in [0, âˆž].
        # style_embed = F.relu(style_embed)

        return style_embed, alignments


class ScaledDotProductAttention(
    nn.Module):  # https://github.com/jadore801120/attention-is-all-you-need-pytorch/blob/master/transformer/Modules.py
    ''' Scaled Dot-Product Attention '''

    def __init__(self, query_dim, key_dim, ref_attention_dim, ref_attention_dropout):
        super().__init__()
        self.dropout = nn.Dropout(ref_attention_dropout) \
            if ref_attention_dropout > 0 else None

        self.d_q = query_dim
        self.d_k = key_dim

        self.linears = nn.ModuleList([
            LinearNorm(in_dim, ref_attention_dim, bias=False, w_init_gain='tanh') \
            for in_dim in (self.d_q, self.d_k)
        ])

        self.score_mask_value = -1e9

    def forward(self, q, k, v, mask=None):
        q, k = [linear(vector) for linear, vector in zip(self.linears, (q, k))]
        alignment = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)  # [N, seq_len, ref_len]
      
        if mask is not None:
            alignment = alignment.masked_fill_(mask == 0, self.score_mask_value)
            
        attention_weights = F.softmax(alignment, dim=-1)
        attention_weights = self.dropout(attention_weights) \
            if self.dropout is not None else attention_weights

        attention_context = torch.bmm(attention_weights, v)  # [N, seq_len, prosody_embedding_dim]

        return attention_context, attention_weights


class LinearNorm(torch.nn.Module):
    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):
        super(LinearNorm, self).__init__()
        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)

        torch.nn.init.xavier_uniform_(
            self.linear_layer.weight,
            gain=torch.nn.init.calculate_gain(w_init_gain))

    def forward(self, x):
        return self.linear_layer(x)
